<< 배당금 프로젝트 >>

=============================== << 스크래핑 >> ===============================

< 웹 스크래핑 >
: HTML 문서를 받음 -> 문서를 파싱 -> 필요한 데이터 추출 하는 과정.
  robots.txt에 정의된 규칙을 준수해 스크래핑 해야하고
  요청 서버에 무리가 가지 않는 선에서 요청을 해야한다. (요청의 양, 요청 텀 적절히 설정해야함.)

- 스크래핑을 위해서는 주로 java보다는 파이썬을 사용한다. (파이썬에서 지원되는 라이브러리나 언어 특성상 스크래핑 코드가 간결하기 때문.)

- 데이터를 긁어다 쓰는 게 문제가 되는 이유
  1. 아무 사이트에서나 데이터를 가져오면 안된다.
     데이터는 회사의 중요 자산이기에 소송에 휘말릴 수도 있기 때문이다.

  2. 웹 서버의 부하 측면 문제 발생
     : 스크래핑 서비스는 내 서버(A)에서 다른 서버(B)로 요청을 보내고
	   해당 요청에 대한 응답인 html 문서를 받아오는 것으로 시작한다.
	   이 때 B서버는 서버에서 A요청에 대한 응답을 내려주기 위해 내부적으로 로직을 수행하게 된다.
	   요청 수가 적으면 문제가 되지 않겠지만 요청 수가 몰리게 되면 당연히 요청을 받는 B서버는 부하가 갈 수 밖에 없다.
	   우리가 스크래핑을 위해 요청을 보낸 탓에 B서버에 부하가 가게 되면
	   해당 서비스를 이용중인 다른 사용자에게의 응답이 지연되거나 응답을 받지 못할 수 있다.
	   최악의 경우는 B서버가 트래픽을 견디지 못해 서버가 다운될 수도 있다.
	   (=디도스공격: 서버에 악의적으로 다량의 요청을 보내 트래픽을 폭주시켜 서버가 견디지 못해 다운되도록 하는 공격방식)
	   따라서 B서버 입장에서는 정상적이지 않은 트래픽을 막아야 한다.
	   비정상적인 트래픽이 감지되면 요청을 보낸 IP가 더 이상 요청을 보내지 못하도록 차단해버리기도 한다.
	   따라서 스크래핑을 할 때는 요청을 보내는 B서버에 부하가 가지 않을 정도로만 진행해야 한다.
	   을 DB에 저장한다.

- 긁어와도 되는 데이터인지 확인하는 방법
  : 일반적으로는 robots.txt 문서에 해당 규칙을 정의해 놓는다.
    (웹사이트의 /(root)경로에서 확인가능.
	 ex> google.com/robots.txt)
    트래픽이 제한되어 있거나 다른 서비스에서 데이터를 가져가길 원치 않는 경우에는
	해당 파일에 Disallow 라고 해서 어떤 경로의 접근을 막는지 명시되어 있다.
	- User-agent: * -> 유저 에이젼트에 * 표시가 있는 경우는 해당 규칙이 모든 robots에 대해 적용된다는 의미이다.
	- Disallow : 해당 경로의 접근 비허용.
	- Allow :해당 경로의 접근 허용.
  : robots.txt 문서가 없는 사이트도 있는데 없더라도 스크래핑을 하려면 보수적인 정책을 가지고 진행해야 한다.


스크래핑은 무조건 데이터를 가져오는 게 아니라 꼭 주의사항을 지키면서 가져와야 한다.
우리가 가져오려는 정보는 웹브라우저에서 랜더링된 그래픽 정보가 아닌 실제 웹 사이트가 가진 정보들이다.
이를 내가 사용하려는 형태로 가공해 사용하는 것이 목적이다.

예시)
원하는 회사 목록이 있다.
이 떄 회사들 리스트를 만들어 두고 해당 회사들의 채용공고가 올라오는지를 매번 웹페이지에 들어가 확인하는 것은 번거롭다.
그렇기에 내가 원하는 회사들의 채용공고만 모아서 한번에 다 볼 수 있도록 하고싶다 또는 새로운 채용 공고가 뜨면 그때만 알림을 주면 좋겠다 라는 니즈가 생길 수 있다.
이 때 웹 사이트들의 정보를 긁어 모으는 것을 웹 스크래핑이라 한다.
하지만 이렇게 무조건적으로 웹 사이트들에서 데이터를 긁어오는 것은 문제가 될 수 있다.


< 스크래핑 사용 이유 & 활용분야 >
국내 주식정보 같은 경우는 공공데이터 API를 통해서 데이터가 제공되는 경우가 많다.
해당 API를 활용해서 개발하게 되면 단순 주식정보를 제공하기 위한 서비스 구현은 매우 쉽다.
이는 매우 기본적인 부분이기에 중요하다.

그럼에도 불구하고 스크래핑이라는 번거로운 작업을 하는 이유는 스크래핑을 활용하면
추후 관련 많은 서비스를 만들 수 있기 때문이다.
쇼핑몰의 정보를 스크래핑해서 가격정보를 비교하거나
특정 카테고리에 해당하는 물품을 모아서 보여주거나
여러 회사의 취업공고를 모아서 보여주는 등의 서비스가 가능하게 된다.
또한 업무자동화에서도 유용하게 사용 가능하다.
많은 핀테크 회사들에서도 사용자의 정보 제공 동의를 받아서
사용자 대신 공공기관에 대해 스크래핑 해오고
사용자가 일일이 서류를 발급받아 제공해야하는 번거로움을 줄이는데 활용하기도 한다.


< 스크래핑을 위한 웹사이트 분석 >
1. 야후 파이낸스 웹페이지 접속
   -> 코카콜라 배당금 정보를 가져오기 위해 'coke'검색 > Historical Data 확인
   : https://finance.yahoo.com/quote/COKE/history/

2. robots.txt 문서 확인
   : https://finance.yahoo.com/robots.txt
   -> /quote 경로의 disallow 여부 확인.
      : 해당 경로를 허용하지 않는다는 정보 없음을 확인.

3. 추출할 데이터의 html상 위치 확인.
   - html 확인하기 : 웹페이지 우클릭 > 검사 > Elements 창을 켜고 Dividend 정보에 커서를 가져가 html상에 해당 데이터의 위치 확인.
   - url이 가지는 정보 확인 : 어떤 파라미터가 어떤 정보를 가지는지 확인
							  (아래의 url은 해당정보를 전체 년도에 대해 월별 데이터이다. 배당금 지급여부는 일일데이터보다 월별데이터로 파악이 더 쉽기 때문이다.)
							  https://finance.yahoo.com/quote/COKE/history/?frequency=1mo&period1=99153000&period2=1733789598

배당금데이터 스크래핑 해와 콘솔에 출력하기


< 야후파이낸셜 코카콜라 배당금 정보 스크래핑 구현하기 >
1. 발생오류 : Caused by: org.jsoup.HttpStatusException: HTTP error fetching URL. Status=503, URL=https://finance.yahoo.com/quote/COKE/history/?frequency=1mo&period1=99153000&period2=1733789598
   원인 : 503 오류는 일반적으로 스크랩하려는 웹사이트가 비인간 사용자가 사이트를 탐색하는 것을 원하지 않기 때문에 차단한다는 것을 의미.
   해결방법 : userAgent 정보 추가해 전달.
   참고페이지 : https://stackoverflow.com/questions/51673995/jsoup-http-error-fetching-url-status-503





=============================== << 서비스 api 설계 >> ===============================
< api 인터페이스 설계 >
요청이 들어오면 서버에서는 그에 대한 응답 내려주거나 응답이 없다면 에러를 내려줄 수 있다.
이 때 어떻게 요청을 보내고 어떻게 응답을 줄 지 결정하는 것이 api 인터페이스 설계이다.
- 요청 : GET /dividend/{company}
- 응답 : {
            companyName: ...,
            dividends: [...]
        }

< 배당금 api 설계 >
1. 특정 회사의 배당금 조회 (핵심, 기본)
   - 방법1. 사용자접근시 요청 : 클라이언트가 해당 데이터에 접근할 때(회사를 클릭할 때) 클라이언트에서 서버로 요청.
   - 방법2. 클라이언트 캐싱 : 캐싱을 통해 회사 데이터를 클라이언트에 저장해두고 필요할 때 꺼내쓸 수 있음.
                          서버까지 요청을 보내고 응답을 받는 과정 생략 -> 속도향상
                          해당 데이터를 저장해야하는 만큼 클라이언트에 추가 저장공간 필요.
                          (속도와 메모리공간은 트레이드오프관계를 지니게됨.)

   - 구현동작 1. input으로 저장할 회사의 ticker를 받는다.
             2. 이미 저장 되어있는 회사의 ticker일 경우 오류 처리.
             3. 받은 ticker의 데이터를 야후 파이낸스에서 스크래핑.
             4. 스크래핑 데이터가 조회되지 않는 경우 오류 처리
             5. 스크래핑한 회사의 메타 정보와 배당금 정보를 각각 DB에 저장.
             6. 저장한 회사의 메타 정보를 응답으로 내려줌.

   요청 : GET /finance/dividend/{companyName}
   응답 : {
           companyName: ..., // 회사명
           dividends: [
                        {
                            date: "2022.3.21"
                            price: "2.00"
                            ...
                        },
                        ...
           ]  // 배당금정보 (다건이므로 리스트형태)
          }

2. 배당금검색 (자동완성기능 추가)
   : 티커(회사코드) 또는 회사명으로 검색가능. (우리는 회사명으로 검색가능)
   : 키워드를 입력할 때마다 자동완성 api 호출.
     (키워드에 적합한 모든 회사정보를 내려줄 필요는 없다.
      적당히 자동검색화면에 출력될 정도만큼의 데이터만 내려주면 된다.
      현재 프로젝트에서는 0 ~ 15개 정도.)
   요청 : GET /company/cutocomplete?keyword=O
   응답 : {
            result: ["O", "OAS", ...]
        }

3. 회사 리스트 조회
   요청 : GET /company
   응답 : {
            result: [
                {
                    companyName: "좋은회사"
                    ticker: "GOOD"
                },
                ...
            ]
        }

4. 배당금 데이터 저장 (관리자기능)
   : 배당금 조회를 하려면 서버의 DB에 해당 데이터가 저장되어 있어야한다.
     따라서 배당금 정보를 조회하는 api가 존재해야 한다.
     이는 고객용이 아닌 서비스 관리자를 위한 기능이다.
     모든 파이낸스 정보를 가져와서 저장하지는 않는다.
     (스크래핑의 시간도 오래 걸리고 데이터의 규모가 너무 커진다.
      따라서 원하는 몇개의 회사의 정보만 스크래핑하고 서비스를 제공하는 컨셉으로 잡는다.)
   요청 : POST /company
         {
            ticker: "GOOD"
         }
   응답 : {
            ticker: "GOOD",
            companyName: "좋은회사",
            ...
        }

4. 배당금 데이터 삭제 (관리자기능)
   요청 : DELETE /company?ticker=GOOD
   응답 : 추가적인 응답값을 내려주지 않고, 응답상태코드로 삭제여부를 전달한다.

5. 회원 API (추후 설계)
   : 인증정보를 어떻게 주고 받는지에 대한 이해가 필수적으로 필요하다.
   - 회원가입
   - 로그인
   - 로그아웃





=============================== << DB 설계 >> ===============================
< DB 설계 시 고려할 점 >
- 저장되는 데이터의 타입
- 데이터의 규모
- 데이터의 저장 주기 (영구적으로 저장되어야 할 데이터도 있지만 주기적으로 지워줘야 하는 데이터도 존재.)
- 데이터의 읽기와 쓰기 연산의 비율
- 속도 vs 정확성 우선순위 결정
- READ 연산 시 어떤 컬럼을 기준으로 읽어올 것인지(index 설계시 필요)
- 키 생성 방법 (자동증가 등)
- 예상 트래픽의 정도
- 파티션 구성 전략
...

< 배당금 DB 설계 >
- 배당금 테이블에서 회사정보를 ticker 컬럼이 아닌 company_id를 저장하는 이유
  : company_id를 이용해 어떤 회사의 배당금 정보인지를 알 수 있다.
    이 때 회사 테이블의 ticker도 회사를 구분하는 유니크 키 인데 company_id를 사용하는 이유는
    회사 정보를 가지고 매칭되는 배당금 정보를 찾을 때 회사정보에 대한 비교 연산을 수행하게 되는데
    비교 연산 시 문자열보다 숫자끼리의 연산이 더 빠르기 때문이다.

- 분산 데이터베이스
  : 데이터베이스도 서버라는 컴퓨터에서 데이터 저장을 하고 연산을 처리한다.
    따라서 서버에 하드디스크 용량에 대한 제한이 있으니 데이터베이스에도 용량 제한이 존재하게 된다.
    그렇기 때문에 서비스 운영 환경에서는 한 서버에 모든 데이터를 저장하지 않고
    여러 서버에 분산시켜 데이터를 저장한다.
    하지만 요청에 대한 응답을 내려줄 때는 마치 하나의 서버처럼 동작한다.
    (원하는 데이터가 있는 DB를 찾아 해당 DB에 접근해 데이터 가져오거나 조작.)
    이런 경우에는 pk를 자동증가하면 안되고 따로 생성해야 한다.
    (매우 흔한 경우, pk 충돌 해결하기 위한 다양한 방법 존재.)

- 데이터를 분산하는 이유
  1. 서버에 하드디스크 용량에 대한 제한 존재.
     -> 용량이 부족해도 서버를 추가해 보완가능.


< h2 DB 설정정보 >
- application.yml 에 작성한다.

spring:
  application:
    name: dayone-financial

    h2:
      console:
        enabled: true // h2 DB console 사용가능
        path: /h2-console // h2 DB console 접근경로

    jpa:
      hibernate:
        ddl-auto: create // 자동 테이블 생성 (프로젝트 실행,종료시마다 테이블 생성->삭제, 따라서 개발 초기단계에서만 사용해야함.)
        use-new-id-generator-mappings: false // id생성전략 (스프링버전에 따라 설정값 상이함.)
      show-sql: true // JPA로 자동 생성되는 쿼리출력
      defer-datasource-initialization: true // 데이터 초기화 옵션 설정(스프링부트 2.5 이상부터 sql 스크립트로 데이터 초기화 옵션을 설정가능. )

    datasource: // h2 DB를 사용하기 위한 사용자정보
      driver-class-name: org.h2.Driver
      url: jdbc:h2:mem:dividend;DB_CLOSE_DELAY=-1
      username: sa
      password:





=============================== << 프로젝트 구조생성 >> ===============================
< Controller >


< Entity >


< Repository >


< Model >
- Entity와 Model 클래스를 분리하는 이유
  코드는 본래 자신의 역할에만 충실해야 한다.
  한 곳에서 여러 역할을 수행하게 되면 코드가 복잡해지고
  예상치못한 사이트 이펙트가 발생할 가능성도 높아진다.
  이는 결국 유지보수가 어려워지고 장기적으로는 어떤 문제가 생길지 모르는 코드를 안고 가는 것이다.
  Entity는 DB와 직접적으로 매핑되기 위한 클래스이다.
  따라서 Entity 인스턴스를 service 코드 내부에서 데이터를 주고 받기 위한 용도로 사용하거나 변경하는 로직이 들어가게 되면
  Entity 클래스의 원래 역할의 범위를 벗어나게 된다.
  그래서 모델과 엔티티 클래스를 따로 정의해 역할을 구분한다.

1. @Data
  : getter, setter, toString, equals, hashcode, requiredArgsConstructor 어노테이션을 포함한다.
  - 해당 어노테이션 사용에는 주의해야한다.
  - 불필요한 어노테이션이 포함되어있다면 사용을 지양해야 한다.
    예를 들면 클래스 외부에서 멤버변수를 임의로 변경이 허용되지 않는 경우에는 @Setter 어노테이션이 포함되지 않도록 해야한다.
    이 때 @Data 어노테이션을 사용하게 되면 외부에서 멤버변수에 접근해 임의 변경이 가능해지게 되므로
    @Data 어노테이션을 사용하지 않고 다른 필요 어노테이션들을 Entity 때 처럼 단순 나열해야 한다.
  - equals() 같은 메서드도 이후에 객체 비교가 중요한 연산에서 내가 원하는대로 동작하는지의 여부를 확인해줘야 한다.
    예를 들어 강의 객체를 생성한다고 했을 때 과목번호, 과목명만 같으면 동일 과목이라고 판단하고 싶은데
    자동생성된 로직에서는 과목번호, 과목명, 강사명까지 같다고 처리한다면 판단 결과가 당연히 다를 수 밖에 없다.
    따라서 객체 간 비교 연산이 중요한 로직에서는 equals()와 hashcode() 메서드를 직접 구현하거나
    @equals 어노테이션을 통해 자동생성 하더라도 생성된 비교연산 로직이 내가 의도한대로 잘 동작하는지 확인하는 과정이 필요하다.

2. @Builder
   : 디자인 패턴 중 빌더패턴이 존재한다.
     빌더패턴을 해당 클래스에서 사용할 수 있게해주는 어노테이션.
   - 예를 들어 Coffee 클래스가 존재한다.
     이 때 모든 arguments를 가지는 생성자를 통해 americano 인스턴스를 생성하려 한다.
     하지만 이의 경우에는 coffee 생성자의 파라미터 값들이 어떤 값을 의미하는지 파악이 어렵다.
     따라서 번거롭게 Coffee 클래스에서 필드를 확인하거나 인자를 잘못 전달하는 실수의 여지가 많아지는 코드가 된다.
     Coffee americano = new Coffee(1, 150, null, 1, false);

     이럴 때 빌더패턴을 적용하면 아래와 같이 코드를 정의할 수 있다.
     아무리 많은 많은 멤버 변수와 인스턴스를 초기화 하더라도
     각 숫자의 값이 무엇을 의미하는지 정확히 지정해 넣어줄 수 있기에 헷갈릴 여지가 줄어든다.
     그리고 생성자의 몇번쨰에 어떤 인자를 넣어야하는지 순서가 중요하지 않게 된다.
     Coffee americano = Coffee.builder()
                              .shots(1)
                              .water(150)
                              .syrup(1)
                              .build();

     또한 생성자는 사용하지 않는 값이라도 해당 인자의 자리에 null 값을 넣줘야 했는데
     빌더패턴을 사용하게 되면 사용하지 않는 변수는 제외하고 필요 변수만 초기화 가능하다.
     물론 생성자도 오버로딩을 통해 전달 파라미터의 개수를 다르게 할 수 있다.
     하지만 빌더패턴만큼 직관적으로 인스턴스를 생성할 수는 없다.





=============================== << YahooFinanceScraper.java >> ===============================
public class YahooFinanceScraper {
    private static final String URL =
        "https://finance.yahoo.com/quote/COKE/history/?frequency=1mo&period1=99153000&period2=1733789598";
    public ScrapedResult scrap(Company company) { ... }
}

1. 전역변수
   - url을 멤버변수로 뒀을 때의 이점.
   : 위의 간략화한 소스코드와 같이 scrap() 메서드에서 사용할 url 정보를 멤버변수로 빼놓았다.
     일단 유지보수 관점에서 추후 url 정보를 찾기도 쉽고 수정도 어렵지 않다는 이점이 있지만,
     메모리 관점에서의 이점도 존재한다.

     우리가 생성자를 통해 인스턴스를 초기화하는 순간 해당 인스턴스는
     프로그래밍이 사용할 수 있는 메모리 공간 중 Heap에 저장공간을 할당받는다.
     따라서 클래스의 멤버변수로 생성한 값들은 Heap에 저장되게 된다.
     떄문에 scraper.scrap() 메서드가 호출될 때 마다
     Heap의 scraper 저장공간에 있는 url 데이터를 참조해 사용 가능하다.
     Scraper scraper = new Scraper(); // Heap 영역에 저장

     하지만 클래스 내부에 정의해준 메서드에서 사용하는 지역변수들은 Heap이 아닌 Stack이라는 별도의 다른 공간에 저장된다.
     stack 영역은 함수가 호출될 때 메모리가 할당되고, 종료 시 할당 해지가 된다.
     또한 동일 메서드 재호출 시 각각 다른 영역을 할당받게 된다.
     때문에 서로간의 데이터는 공유되지 않는다.
     따라서 scrap() 메서드 안에서 url을 선언해 사용한다면 stack영역의 scrap() 호출 부분마다 url 변수가 생성되어
     해당 영역만큼 저장공간을 더 차지하게 된다.
     scraper.scrap(); // Stack 영역에 저장

    그렇다고 모든 데이터를 메모리 절약을 위해 전역변수로 생성해 사용하는 것은 불가능하다.
    메모리 영역은 제한되어있기 때문이다.
    그래서 해당 영역을 초과하게 되면 에러가 발생하면서 서비스가 죽게된다.
    하지만 자바 프로그램일 계속 동작하게 되면 Heap 공간에 데이터가 당연히 쌓일 수 밖에 없다.
    그래서 자바 내부에서는 주기적으로 해당 공간에 사용되지 않고 있는 인스턴스들을 비워주는
    가비지콜렉션이라는 작업을 수행한다.
    따라서 모든 변수를 멤버 변수로 만들면 Heap 영역의 메모리 공간은 더 빠르게 찰 수 밖에 없고
    가비지콜렉션은 더 자주 수행되어 성능이 저하되는 문제가 발생할 수 있다.

2. static변수
   : url은 값이 바뀌면 안되는 상수값이므로 static final로 선언한다.
   - final : 초기화 이후 값 변경불가

   클래스 인스턴스를 사용할 때 하나만 생성해 사용하지 않고 여러 인스턴스를 생성해 사용할 수 있다.
   그러면 클래스 인스턴스들이 생성될 때 마다 각각 Heap 영역을 할당받는다.
   그렇기에 static이 아닌 일반 멤버변수로 사용하게 되면 각 인스턴스들은 내부에 url 변수를 각각 갖게된다.

   하지만 url을 static 변수로 선언하게 되면
   url값이 static 변수를 저장하기 위한 static Area에 따로 저장되게 된다.
   그래서 클래스 인스턴스들이 Heap 영역에 각각 생성되더라도 인스턴스들에서 사용되는 url은 모두
   static Area에 있는 url을 참조하게 된다.

   따라서 static 변수는 모든 인스턴스에 접근해 사용 가능하기에
   잘 이해하고 사용하지 않으면 굉장히 많은 버그들의 원인이 될 수 있다.

- 가비지콜렉션 (Garbage Collection)
  1. 실행순서
     : 메모리 할당 후 해제가 안되면 메모리 누수 발생
        -> 수동으로 개발자가 직접 해제할 경우, 올바르게 해제되지 않게되면 버그 발생 가능성이 높음.
        -> 가비지 컬렉터는 동적으로 할당된 메모리 영역 중 더 이상 쓰이지 않는 영역을 찾아내 해제.
  2. 가비지콜렉션은 자바 프로그램의 성능에 굉장히 직접적인 영향을 주는 동작이다.
     가비지콜렉션이 자주 수행될수록 자바 프로그램은 느려질 수 밖에 없다.
     물론 가비지콜렉션이 실행되더라도 성능에 영향을 최소화하기 위한 알고리즘들이 계속 발전하고 있다.

- 현재까지 scraper 서비스를 생성했다.
  만약 스크래핑할 사이트가 변경되거나 다른 스크래핑할 사이트가 추가되어 더 많은 정보를 취급하게 될 수도 있다.
  이 때 서비스에 의존하게 되면 추후 변경시에 많은 공수가 필요하다.
  따라서 코드의 재사용성과 확장성을 위해 범용성있는 스크래퍼 인터페이스가 필요하다.





=============================== << CompanyService.java >> ===============================
1. getAllCompany() : 회사 목록 조회 서비스
   - 조회할 회사의 개수는 서비스에서는 수백, 수천개가 넘는다.
     api 호출에 대한 응답 결과로 이 모든 정보를 조회하는 것은 옳지 않다.
     요청을 주고 받는 데이터의 수가 클수록 당연히 네트워크의 대역폭도 더 많이 사용해야 하므로
     서비스 전체에 악영향을 줄 가능성도 높아진다.
     또한 그렇게 많은 회사 목록을 받는다 하더라도
     어짜피 클라이언트에서 한번에 보여줄 수 있는 아이템의 개수는 한정되어있다.
     따라서 적당한 수의 회사 정보만 조회하도록 한다.
   - Pageable : 페이지 기능을 지원한다.
              : import org.springframework.data.domain.Pageable;
              : controller에서 파라미터로 Pageable을 받을 수 있도록 한다.
                클라이언트에서 페이징 관련 옵션을 추가해 api를 호출할 수 있게 된다.
                Pageable이 임의로 변경되는 것을 막기위해 final로 선언한다.
                service에서 JPA repository의 findAll() 메서드 호출 시 인자로 Pageable 객체를 전달한다.
                단, 반환값은 List<Entity> 타입이 아닌 Page<Entity>로 변경되어야 한다.
              : 페이지 관련 옵션
                응답시 회사목록에 추가로 하단에 페이지에 대한 정보도 함께 제공한다.
                페이지의 사이즈(한 페이지에 출력할 데이터 수)는 default로 20.
                따라서 사이즈 변수를 파라미터로 넘겨 사이즈 조절도 가능하다.
                query param으로 사이즈=3, 페이지는 0번째 페이지 출력하도록 전달 url
                - http://localhost:8080/company?size=3&page=0
              {
                  "content": [
                      {
                          "id": 1,
                          "ticker": "MMM",
                          "name": "3M Company"
                      },
                      ...
                  ],
                  "pageable": {
                      "sort": {
                          "empty": true,
                          "sorted": false,
                          "unsorted": true
                      },
                      "offset": 0,
                      "pageNumber": 0,
                      "pageSize": 20,
                      "paged": true,
                      "unpaged": false
                  },
                  "last": true,
                  "totalElements": 4,
                  "totalPages": 1,
                  "size": 20,
                  "number": 0,
                  "sort": {
                      "empty": true,
                      "sorted": false,
                      "unsorted": true
                  },
                  "first": true,
                  "numberOfElements": 4,
                  "empty": false
              }
              : 페이지 기능은 흔하지만 서비스의 핵심적인 기능은 아니다.
                때문에 직접 구현하려면 많은 공수가 필요하지만 스프링부트에서 지원하는 Pageable을 사용해 구현하게 되면
                단순한 기능보다 핵심적인 기능에 집중해 더 완성도있는 서비스를 제공할 수 있다.





=============================== << CompanyRepository.java >> ===============================
< Optional로 wrapping된 형태를 return 값으로 사용하는 이유 >
ex) Optional<CompanyEntity> findByName(String name);
    1. NullPointException 방지
    2. 값이 존재하지 않는 경우의 예외처리를 코드적으로 깔끔하게 처리가능





=============================== << Trie 자료구조와 자동완성검색 (CompanyService.java) >> ===============================
< 속도와 메모리의 관계 >
  속도와 메모리 공간은 trade-off 관계이다.
  과거에 비해 메모리의 가격이 싸지고, 크기 또한 비약적으로 늘었다.
  따라서 돈으로 서버나 메모리 공간을 늘릴 수 있다.
  하지만 속도는 돈으로 해결할 수 없다.
  따라서 메모리공간보다 속도를 우선시하는 추세이다.
  하지만 무조건 속도를 우선시하는 것이 베스트는 아니다.
  상황에 맞게 우선시하는 것을 정하는 것이 좋다.


< Trie 자료구조 >
- 트리형태의 자료구조. (탐색속도 빠름, 메모리효율저하)
- 문자열 탐색을 효율적으로 가능.
  1. Trie 시간복잡도 : O(L)
  2. List 시간복잡도 : O(L*N) // L=문자열의길이, N=단어의개수
- 중복저장 X.
- 메모리를 훨씬 많이 차지한다는 단점.
  : 한 노드의 하위노드는 모든 문자열에 대한 각 노드마다 하위 노드들을 다 가질 수 있다.
    각 노드는 다음 노드를 가리키는 메모리 공간도 할당되어야 한다.

- 제일 앞글자를 최상위노드로 해서 한글자씩 하위노드로 한글자씩 추가된다.
  단어의 마지막 글자는 최하위 노드가 되며, 단어의 끝이라는 의미로 flag 표시를 해준다.
  ex) cat이라는 단어와 cap이라는 단어를 trie 자료구조에 넣게되면
      c,a라는 공통노드를 갖게되고, t,p는 flag표시 처리를 해준다.
  ex2) con, conp라는 단어를 추가하게 되면
       c,o라는 공통노드를 갖게되고 n에 flag, p에 flag 표시를 해주어 해당 word로 끝나는 단어가 있음을 표시한다.
- Java에서 지원하는 Trie 자료구조 구현한 라이브러리를 사용해 표현 가능하다.
- Trie 구조를 직접 구현해보길 추천한다.


< Trie 자료구조와 자동완성 >
자동완성으로 찾고자하는 단어가 ca인 경우
그 아래에 있는 하위 노드들을 가지고와 해당 키워드로 시작하는 단어들을 찾을 수 있다.
중복단어를 저장할 필요도 없고 해당 단어가 몇개씩 존재하는지 알 필요도 없다.
(자연어처리에 최적화 형태)


< Trie에 데이터 저장하기 >
1. 삽입하려는 문자열을 앞에서부터 한 글자씩 가져오기.
2. 트리의 루트부터 적합한 노드 위치를 찾아가며 저장하기.
   : 이미 존재하는 word에 대한 노드가 있다면 해당 노드의 위치로 들어가면되고
     일치하지 않는 노드가 존재한다면 새로운 노드를 생성해 삽입.
3. 마지막 글자까지 삽입되면 isEnd flag로 단어의 끝을 표시하기.


< Trie에서 데이터 검색하기 >
1. input으로 받은 문자열 -> 한글자씩 parsing
2. parsing된 문자를 앞에서부터 비교
3. 해당 문자 노드가 존재하지 않거나, leafNode(isEnd, isWord flag 존재노드)에 도달할 때까지 탐색.
   : 위와같은 탐색으로 단어가 완성되는 경로를 찾게되면
     하위 경로에 존재하는 단어들을 모두 모아서 반환.
     = input으로 시작하는 문자열목록





=============================== << 싱글톤관리 (CompanyService.java) >> ===============================
< @Service과 싱글톤 >
1. 싱글톤패턴
   : 프로그램이 실행되는 동안 1개의 인스턴스만 생성되고 공유해 사용되어야 할 때 사용되는 디자인패턴

2. @Service 어노테이션 적용한 CompanyService
    해당 어노테이션으로 인해 스프링에서 CompanyService 빈은 싱글톤으로 관리된다.
    싱글톤으로 관리된다는 것은
    프로그램이 실행되는 동안 companyService 인스턴스는 하나만 생성되고 공유해 사용한다는 것이다.
    (companyService 인스턴스가 공유되기에 trie 변수 또한 공유가능)

3. 하나의 인스턴스만 사용되는 경우가 아닐 때 발생할 수 있는 문제점.
   : 각 인스턴스는 각각 다른 메모리공간이 할당되어 저장된다.
     add("hello") 메서드를 호출하면 trie 자료구조에 데이터를 저장하는데
     이 trie 변수 또한 당연히 공유되지 않는다.
     따라서 autoComplete1에 데이터를 저장해도 autoComplete2에서 해당 데이터를 가져올 수 없다.(당연)
   ex) AutoComplete autoComplete1 = new AutoComplete(); // 인스턴스1 생성
       AutoComplete autoComplete2 = new AutoComplete(); // 인스턴스2 생성
       autoComplete1.add("hello");
       System.out.println(autoComplete1.get("hello")); // hello 출력
       System.out.println(autoComplete2.get("hello")); // null 출력 (autoComplete1에 hello를 저장했지 autoComplete2에 저장한 것은 아니므로)

4. (추가) 인스턴스를 공유하지 않을 경우 변수 공유를 원할 때.
   : AutoComplete에서 trie변수를 바로 초기화하지 않고
     생성자를 통해 입력받은 trie를 이용해 초기화한다.
     그리고 위와 같이 AutoComplete를 생성하는 쪽에
     Trie trie = new PatriciaTrie(); 로 trie 변수를 생성해
     autoComplete1, autoComplete2 생성자 호출 시 동일한 trie 변수를 인자로 전달한다.
     이렇게되면 인스턴스 하나를 공유하지 않고도 trie 변수를 두 인스턴스에서 공유 가능하다.

   ex) AutoComplete.java
     public class AutoComplete {
        private Trie trie = new PatriciaTrie();
     }
     아래와같이 변경.
     public class AutoComplete {
        private Trie trie;

        public AutoComplete(Trie trie) {
            this.trie = trie;
        }
     }

5. CompanyService의 trie변수 빈으로 관리하기 (AppConfig.java)
   : 스프링 서버가 초기화되면서 trie 인스턴스도 빈으로 초기화 될 수 있게 AppConfig 코드를 작성해야한다.
     CompanyService.java에서 trie 변수를 초기화해도 되지만 (private final Trie trie = new PatriciaTrie<>();)
     trie 변수는 CompanyService 내에서 하나만 존재해 유지되어야하고
     코드의 일관성을 유지하기 위해 빈으로 trie 변수를 관리한다.
     해당 trie()의 return 타입은 자동완성 키워드에 대한 문자열를 반환해야 하므로 String 타입으로 선언
     AppConfig 에서 생성된 trie 빈이 초기화될 때 companyService의 trie 변수에 주입되면서
     companyService의 trie 인스턴스로 사용될 수 있다.

6. apache 에서 구현된 trie
   : 아주 기본적인 형태의 trie 자료구조라기 보다는 추가 기능을 포함하고있는 구조이다.
     key-value를 함께 저장할 수 있는 기능이 여기에 해당하는 추가기능이다. -> trie.put(keyword, null)
     자동완성 기능만 구현할 것이기 때문에 value값을 넣지 않는다.

7. 의문점 (8번을 통해 해소)
   : 그런데 지금 h2 DB를 사용해 서버 종료 시 데이터를 모두 날려서 해당방법이 유효하지만
     DB에 영구적으로 데이터를 저장하는 경우
     회사 정보를 DB에 추가할 때 trie에 데이터를 저장하고
     trie에서 회사정보를 자동검색하는 경우
     서버를 종료했다가 재시작하면 trie에 데이터가 저장되어있지 않아 검색불가.
     그럼 trie 빈생성시 모든 회사정보를 DB에서 읽어와야하나?
     추후 수백, 수천개의 회사가 생기면 이것도 불가능해보인다.
     따라서 자동검색 시 DB에 해당 키워드에 대한 데이터를 조회해와서 return 해 주어야 하지 않나 라는 의문이 생겼다.

8. Trie 자료구조를 이용한 자동조회 vs DB에서 Like연산을 통한 자동조회
   - Trie
     : 데이터를 trie 변수에 추가로 저장했다.
       데이터를 저장하고 찾는 연산 모두 서버에서 수행된다.
       Trie 자료구를 사용하기 위한 라이브러리를 import 해야한다.
       회사명을 저장할 때 마다 trie에 저장하는 추가 로직을 수행해야 한다.
   - Like 연산
     : 구현은 Trie보다 더 간단하다.
       (회사명을 저장할 때 마다 trie에 저장하는 추가 로직 수행 x, 라이브러리 import필요 x 등)
       다만, 데이터를 찾는 연산이 모두 DB에서 이뤄지기 때문에
       DB에 더 많은 부하가 갈 수 밖에 없다.
       따라서 데이터의 양과 해당 연산이 발생하는 비율 등을 고려해
       DB에 영향을 주지 않을 정도의 트래픽이라면
       해당 방법을 사용해도 괜찮다. (DB에 부하를 줄 정도라면 피해야함.)





=============================== << 스케줄러 Scheduler >> ===============================
: 일정 주기마다 특정 작업을 수행하도록 하는 역할.

1. 멀티 스레드가 아닌 스케줄러 사용하는 이유
  : 일정한 매 시간 마다 수행되어야 하는 작업들은 스레드를 써서 직접 컨트롤 할 수도 있지만
    스레드에 대한 제대로 된 이해 없이 멀티 스레드를 이용하게 되면
    프로그램에 심각한 성능 저하를 가져오거나
    메모리 누수가 발생하는 등의 이슈가 발생할 가능성이 매우 높다.
    그렇기에 개발자가 신경쓰지 않아도 스케줄링 기능을 구현할 수 있도록 지원되는 스케줄러를 사용하게 되면
    훨씬 더 편리하게 스케줄링 기능 사용이 가능하다.

2. 자바 스케줄러에서 실행주기 설정 방법
   1. fixedDelay : 이전 작업 수행이 종료된 시점을 기준으로 일정시간 후에 다음 작업 수행.
   2. fixedRate : 이전 작업 수행이 시작된 시점을 기준으로 일정시간 후에 다음 작업 수행.
                : 작업에 소요되는 수행 시간이 fixedRate에서 지정된 시간보다 길어지는 경우
                  두 작업이 겹쳐서 실행되는 이슈 발생 가능.
   3. cron : 스케줄러 정규 표현식인 cron 표현식을 이용해 수행시간 설정.
           - 초(0-59) / 분(0-59) / 시(0-23) / 일(1-31) / 월(1-12 or JAN-DEC) / 요일(0-6 or SUN-SAT) / 년도(생략가능, 1970-2099)
           사용가능 특수문자 - * : 모든 수
                          - ? : 조건없음(날짜와 요일에만 사용가능)
                          - - : 범위(기간)
                          - , : 특정 여러시간 지정
                          - / : 시작시간과 반복간격
                          - L : 지정할 수 있는 범위의 마지막값(날짜와 요일에만 사용가능)

3. 스프링 @Scheduled 어노테이션 사용
   - 스케줄링을 사용하기 위해 DividendApplication.java에
     @EnableScheduling 어노테이션을 추가해 스케줄링 기능사용 허용 필수.
   - @Scheduled 어노테이션의 cron 속성을 이용해 스케줄링 작업 생성.

4. 스케줄링을 통한 추가 배당금정보 저장 자동화
   : 회사 및 배당금 정보 저장(addCompany) api를 호출해 배당금 정보를 일괄 저장한 이후
     지급되는 배당에 대한 정보도 존재한다.
     이 추가된 새로운 배당정보를 가져와 DB에 저장해야할 필요성이 존재한다.
     이를 자동화하지 않으면 추가 배당금 지급이 발생할 때마다 우리가 가지고 있는 모든 회사정보를 체크하면서
     직접 데이터를 추가해줘야 한다. (한 회사에 추가적으로 들어온 배당금 정보가 있는지 확인하고 있다면 DB에 저장해주기)
     이렇게 서비스를 운영하게 되면 데이터가 늘어날수록 엄청난 인력난에 시달리게 될 것이다.
     또한 수작업으로 하다보면 놓치거나 실수하는 부분이 발생할 수 있다. (휴먼에러발생)
   1. company 테이블에 저장된 회사정보 조회
   2. 회사정보를 추가한 url에 해당하는 html 스크래핑
   3. update된 배당금 내역 존재 시 DB에 추가 저장

5. 배당금 데이터 중복 저장 불가 처리
   - DividendEntity.java에 복합 유니크키 설정 추가.
     (회사id, 해당 날짜에 대한 배당금 중복저장불가.)
     : 회사id, 날짜에 해당하는 복합 유니크키 설정을 걸어주었기 때문에
       회사id, 날짜로 레코드 조회 시
       일반 select where절의 조건으로 데이터를 조회할 때 보다 훨씬 빠르게 조회가능.
       데이터가 많아질수록 index를 걸지않고 데이터를 조회하게 되면 DB 성능에 지장을 주게 된다.
     @Table(uniqueConstraints = {
            @UniqueConstraint(
                    columnNames = {"companyId", "date"}
            )
     })

   - 유니크키 : 일종의 인덱스키이자 제약조건.
               (중복 데이터 저장을 방지하는 제약조건.
               단일 컬럼 뿐 아니라 복합 컬럼 지정 가능.)
             : 유니크키의 디테일한 속성은 DB마다 상이.
             : 유니크키가 걸려있는 상황에서 중복데이터 저장 시 exception 발생.

   - 유니크키 중복 데이터 처리방법
     : 중복 관계없이 데이터를 넣어보고 발생하는 exception을 핸들링해서 해결.
       또는 데이터를 insert할 때 기본 쿼리문 대신 ignore 키워드,
       ON DUPLICATE KEY UPDATE 구문 사용으로 해결가능.

     1. INSERT IGNORE 키워드 사용.
        : INSERT문 실행 시 중복 키 데이터 삽입 발생하면 해당 레코드 무시하고 처리.
        : 일반적으로 사용하는 INSERT문에 IGNORE 키워드 추가
        INSERT IGNORE INTO [TABLE] (COLUMN1, COLUMN2)
        VALUES (VALUE1, VALUE2);

     2. ON DUPLICATE KEY UPDATE 구문 사용.
        : 중복된 키의 레코드가 들어왔을 때
          레코드의 컬럼 값 중에서 KEY UPDATE 쿼리에 정의된 값들에 대해서는 UPDATE 수행.
        INSERT INTO [TABLE] (COLUMN1, COLUMN2)
        VALUES (VALUE1, VALUE2)
        ON DUPLICATE KEY UPDATE(...);

  - 현재 서비스에서 중복 데이터 처리방법
    : 우리 서비스에서는 위의 방법 중 어떤 것도 사용하지 않는다.
      서비스 로직이 쿼리에 직접적으로 들어가는 부분이 특별한 사유로 필요한 게 아닌 이상 선호하지 않아
      DividendRepository에 exist 메서드를 생성해 데이터가 존재하는지 체크해
      존재하지 않는 경우 데이터를 추가하도록 한다.

6. 스케줄링 주기 config 설정으로 셋팅하기.
   - 운영의 편의를 위해 config 설정으로 셋팅
     : 코드에 실행주기를 직접 표현하게되면 스케줄을 변경할 때 마다 코드를 빌드하고 배포하는 프로세스를 거쳐야 한다.
       이는 실제 서비스에서 굉장히 번거롭고 비효율적인 작업이다.
       이렇게 서비스 제공중에 변경될 수 있는 소지가 있는 부분은 config 설정으로 따로 빼주는 것이 좋다.
       그럼 변경 후 배포를 따로 하지 않아도 설정값만 바꿔 재실행해주면 바뀐 설정값이 제공되기 때문이다.

7. 스레드풀 (Thread Pool)
   - 스레드 풀의 필요성
     : 스케줄로 등록되어야하는 기능이 한가지였기에 단순히 @Scheduled 어노테이션을 이용해 구현 가능하지만
       여러 기능이 스케줄로 등록되어야 하는 경우에는 스레드풀을 고려해야 한다.
       스케줄을 등록했을 때 한 작업이 종료되지 않으면 다른 작업은 수행하지 않는다.
       이는 스케줄러가 하나의 스레드로 동작하는 것을 기본으로 때문이다.
       그렇기에 여러 스케줄을 동시 처리하기 위해 스레드풀이 필요하다.
       (스케줄러 자체는 스프링 프로세스의 메인 스레드와는 별도의 스레드로 동작한다.)

   - 스레드 풀
     : 여러 개의 스레드를 유지,관리 수행
     : 스레드는 한번 생성시키고 소멸하면 굉장히 많은 리소스를 소모한다.
       따라서 스레드 풀에서는 설정된 크기의 스레드들을 만들어두고
       해당 스레드들을 계속해서 재사용할 수 있도록 관리해준다.

   - 스레드 풀의 적정 사이즈
     : 스레드를 불필요하게 너무 많이 생성하면 메모리를 낭비하게 되고
       너무 적은 스레드를 생성하면 효율성이 떨어지기에
       적절한 스레드 풀의 사이즈 설정이 필요하다.
     : 스레드의 개수는 하드코딩으로 값을 고정시키는 것 보다
       서비스가 실행되는 cpu의 core 개수에 따라 유동적으로 생성되도록 하는 것이 좋다.
     : 어떤 작업을 처리하느냐가 스레드풀의 사이즈를 결정하는 기준이 될 수 있다.
     1. cpu 처리가 많은 경우
        : 일반적으로 스레드가 cpu를 많이 쓰는 작업을 하는 경우
          cpu의 core 개수가 n이라고 했을 때 [n + 1] 만큼의 스레드를 생성해주면
          최적에 가까운 성능을 낼 수 있다고 본다.
     2. I/O 작업이 많은 경우
        : I/O 작업 처럼 블로킹이 많은 작업을 처리할 경우에는
          [core수 * 2]로 2배 만큼의 스레드를 생성해 사용하기도 한다.






=============================== << DB index >> ===============================
- 데이터베이스 인덱스 공부 필수 권장.

1. 유니크키 설정
   - DividendEntity.java
     : 회사id, 날짜에 해당하는 복합 유니크키 설정을 걸어주었기 때문에
       회사id, 날짜로 레코드 조회 시
       일반 select where절의 조건으로 데이터를 조회할 때 보다 훨씬 빠르게 조회가능.
       데이터가 많아질수록 index를 걸지않고 데이터를 조회하게 되면 DB 성능에 지장을 주게 된다.

2. index 사용
   - index 사용 시 성능향상
     but index를 건다고 무조건 조회 성능이 빨라지는 것은 아니다.

3. Cardinality 와 index
   : Cardinality가 낮은 데이터에 대해서는 해당 컬럼에 index를 걸어도 성능이 빨라지지 않는다.
     Cardinality가 높은 데이터에 대해서는 중복 데이터가 적으므로 해당 컬럼에 index를 걸어도 성능이 빨라질 가능성이 높다.
     따라서 많이 조회하는 컬럼이라고 해서 index를 거는 것은 좋은 방법이 아닐 수 있다.
     index를 설정할 때는 Cardinality 외에도 selectivity(선택도), 해당 쿼리의 호출 빈도 등 고려해야할 상황들이 많다.
     따라서 index를 거는 일은 신중해야 한다.

   - 중복된 데이터가 많은 상태 = Cardinality가 낮은 데이터
   - 중복된 데이터가 적은 상태 = Cardinality가 높은 데이터
   ex) 성별 컬럼의 경우 남자, 여자 두 가지의 경우를 지닌다.
       이런 경우 데이터의 중복이 많은 상태로 Cardinality가 낮은 데이터이다.
       따라서 이런 경우는 index를 걸어도 효율적으로 동작하지 않는다.





=============================== << Thread.sleep() vs java제공 wait() 메서드 >> ===============================
ScraperScheduler.java의 yahooFinanceScheduling() 메서드

1. 연속적으로 스크래핑 대상 사이트 서버에 요청을 날리지 않도록 일시정지
   : 연속적인 요청을 하게되면 서버에 부하발생가능 -> 텀을 줘야함.
     thread sleep을 걸어서 잠시 정지하도록 코드 작성함.

2. Thread.sleep(ms)
   : 실행중인 스레드 잠시 멈춤. (단순일시정지, 스스로 깸.)
   - InterruptedException 발생가능
     : 인터럽트를 받는 스레드가 blocking 될 수 있는 메소드 실행 시 발생.
       적잘한 조치를 취하지 않으면 스레드가 정상적으로 종료되지 못하는 상태에 빠질 수 있다.
       ex) Thread.currentThread().interrupt(); 수행
3. wait()
   : 스레드를 대기 상태에 빠뜨림.
   - notify(), notifyAll() 메소드 호출전까지 자동으로 깨지 않음.
   - 다른메서드에서 호출가능해 스레드간 통신에 주로 사용.





=============================== << 스레드 Thread >> ===============================
- 스레드와 프로세스 차이 필수 이해.
- 스레드 간 공유 메모리 공간의 종류.
- 스레드 다량 생성과 처리속도.
- 동기 비동기 차이.
- 뮤텍스, 세마포어 차이.





=============================== << 환경설정 >> ===============================
lombok을 추가해줬는데 실행이 안되면
File > Settings > Build, Exception, Deployment > Compiler > Annotation Processors > Enable annotation processing 이 체크되어있는지 확인





=============================== << 개발관련 조언(?) >> ===============================
- 서버개발 시 중요한 것 중 하나는
  내가 개발하는 서비스가 어떻게 돌아가고 있는지 잘 파악하고 있는 것은 당연하고
  내 서비스가 연동되어 돌아가고 있는 다른 서비스, 서버에 어느정도의 영향력을 갖고 있는지 파악하는 것도 굉장히 중요한 이슈 중 하나이다.
  (내가 개발한 서비스를 어디서 호출하는지, 내가 호출하는 서비스는 어떤 서버인지 가능한 파악하고 있는게 장애가 발생했을 때 당황하지 않고 빠르게 수습 가능하다.)

- 제로베이스 배당금프로젝트 스크래핑 구현01 강의 -> API 보는 법

- 내가 다루는 데이터의 속성을 파악할 수 있어야 한다.
  추후 어떤 도메인의 데이터를 다루게 되더라도 해당 데이터의 특징을 직접 파악하고 그에 맞는 개발을 하고 운영방침을 세워나갈 수 있다.
  내가 담당하는 도메인을 잘 파악하는 것도 중요한 과업이다.

- url 경로는 어떤 데이터를 조회, 조작할 것인지 예측 가능한 정보를 표시하면 좋다.
  무작위로 생성하면 해당 api를 호출하는 사용자가 해당 api가 어떤 기능을 하는지 알 수 없기 떄문이다.

- HTTP 프로토콜 메서드 차이 (꼭 한번 읽어볼 것)
  https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.1
  - GET, POST 차이!!!

- api 구현보다 설계가 중요한 이유.
  구현에 있어서 결과도 중요하지만
  초기에는 왜 이렇게 구현했는지 이해하고 설명할 수 있는 것이 훨씬 중요하다.
  이 과정이 없으면 프로젝트를 찍어내는 것 밖에 안된다.
  면접 과정에서도 왜 이렇게 구현했는지, 왜 해당 라이브러리를 사용했는지, 왜 api는 해당 방식으로 설계했는지 등
  어디까지 고민해보고 코드를 구현했는지가 더 중요하다.

