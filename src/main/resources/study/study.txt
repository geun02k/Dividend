<< 배당금 프로젝트 >>

=============================== << 스크래핑 >> ===============================

< 웹 스크래핑 >
: HTML 문서를 받음 -> 문서를 파싱 -> 필요한 데이터 추출 하는 과정.
  robots.txt에 정의된 규칙을 준수해 스크래핑 해야하고
  요청 서버에 무리가 가지 않는 선에서 요청을 해야한다. (요청의 양, 요청 텀 적절히 설정해야함.)

- 스크래핑을 위해서는 주로 java보다는 파이썬을 사용한다. (파이썬에서 지원되는 라이브러리나 언어 특성상 스크래핑 코드가 간결하기 때문.)

- 데이터를 긁어다 쓰는 게 문제가 되는 이유
  1. 아무 사이트에서나 데이터를 가져오면 안된다.
     데이터는 회사의 중요 자산이기에 소송에 휘말릴 수도 있기 때문이다.

  2. 웹 서버의 부하 측면 문제 발생
     : 스크래핑 서비스는 내 서버(A)에서 다른 서버(B)로 요청을 보내고
	   해당 요청에 대한 응답인 html 문서를 받아오는 것으로 시작한다.
	   이 때 B서버는 서버에서 A요청에 대한 응답을 내려주기 위해 내부적으로 로직을 수행하게 된다.
	   요청 수가 적으면 문제가 되지 않겠지만 요청 수가 몰리게 되면 당연히 요청을 받는 B서버는 부하가 갈 수 밖에 없다.
	   우리가 스크래핑을 위해 요청을 보낸 탓에 B서버에 부하가 가게 되면
	   해당 서비스를 이용중인 다른 사용자에게의 응답이 지연되거나 응답을 받지 못할 수 있다.
	   최악의 경우는 B서버가 트래픽을 견디지 못해 서버가 다운될 수도 있다.
	   (=디도스공격: 서버에 악의적으로 다량의 요청을 보내 트래픽을 폭주시켜 서버가 견디지 못해 다운되도록 하는 공격방식)
	   따라서 B서버 입장에서는 정상적이지 않은 트래픽을 막아야 한다.
	   비정상적인 트래픽이 감지되면 요청을 보낸 IP가 더 이상 요청을 보내지 못하도록 차단해버리기도 한다.
	   따라서 스크래핑을 할 때는 요청을 보내는 B서버에 부하가 가지 않을 정도로만 진행해야 한다.
	   을 DB에 저장한다.

- 긁어와도 되는 데이터인지 확인하는 방법
  : 일반적으로는 robots.txt 문서에 해당 규칙을 정의해 놓는다.
    (웹사이트의 /(root)경로에서 확인가능.
	 ex> google.com/robots.txt)
    트래픽이 제한되어 있거나 다른 서비스에서 데이터를 가져가길 원치 않는 경우에는
	해당 파일에 Disallow 라고 해서 어떤 경로의 접근을 막는지 명시되어 있다.
	- User-agent: * -> 유저 에이젼트에 * 표시가 있는 경우는 해당 규칙이 모든 robots에 대해 적용된다는 의미이다.
	- Disallow : 해당 경로의 접근 비허용.
	- Allow :해당 경로의 접근 허용.
  : robots.txt 문서가 없는 사이트도 있는데 없더라도 스크래핑을 하려면 보수적인 정책을 가지고 진행해야 한다.


스크래핑은 무조건 데이터를 가져오는 게 아니라 꼭 주의사항을 지키면서 가져와야 한다.
우리가 가져오려는 정보는 웹브라우저에서 랜더링된 그래픽 정보가 아닌 실제 웹 사이트가 가진 정보들이다.
이를 내가 사용하려는 형태로 가공해 사용하는 것이 목적이다.

예시)
원하는 회사 목록이 있다.
이 떄 회사들 리스트를 만들어 두고 해당 회사들의 채용공고가 올라오는지를 매번 웹페이지에 들어가 확인하는 것은 번거롭다.
그렇기에 내가 원하는 회사들의 채용공고만 모아서 한번에 다 볼 수 있도록 하고싶다 또는 새로운 채용 공고가 뜨면 그때만 알림을 주면 좋겠다 라는 니즈가 생길 수 있다.
이 때 웹 사이트들의 정보를 긁어 모으는 것을 웹 스크래핑이라 한다.
하지만 이렇게 무조건적으로 웹 사이트들에서 데이터를 긁어오는 것은 문제가 될 수 있다.


< 스크래핑 사용 이유 & 활용분야 >
국내 주식정보 같은 경우는 공공데이터 API를 통해서 데이터가 제공되는 경우가 많다.
해당 API를 활용해서 개발하게 되면 단순 주식정보를 제공하기 위한 서비스 구현은 매우 쉽다.
이는 매우 기본적인 부분이기에 중요하다.

그럼에도 불구하고 스크래핑이라는 번거로운 작업을 하는 이유는 스크래핑을 활용하면
추후 관련 많은 서비스를 만들 수 있기 때문이다.
쇼핑몰의 정보를 스크래핑해서 가격정보를 비교하거나
특정 카테고리에 해당하는 물품을 모아서 보여주거나
여러 회사의 취업공고를 모아서 보여주는 등의 서비스가 가능하게 된다.
또한 업무자동화에서도 유용하게 사용 가능하다.
많은 핀테크 회사들에서도 사용자의 정보 제공 동의를 받아서
사용자 대신 공공기관에 대해 스크래핑 해오고
사용자가 일일이 서류를 발급받아 제공해야하는 번거로움을 줄이는데 활용하기도 한다.


< 스크래핑을 위한 웹사이트 분석 >
1. 야후 파이낸스 웹페이지 접속
   -> 코카콜라 배당금 정보를 가져오기 위해 'coke'검색 > Historical Data 확인
   : https://finance.yahoo.com/quote/COKE/history/

2. robots.txt 문서 확인
   : https://finance.yahoo.com/robots.txt
   -> /quote 경로의 disallow 여부 확인.
      : 해당 경로를 허용하지 않는다는 정보 없음을 확인.

3. 추출할 데이터의 html상 위치 확인.
   - html 확인하기 : 웹페이지 우클릭 > 검사 > Elements 창을 켜고 Dividend 정보에 커서를 가져가 html상에 해당 데이터의 위치 확인.
   - url이 가지는 정보 확인 : 어떤 파라미터가 어떤 정보를 가지는지 확인
							  (아래의 url은 해당정보를 전체 년도에 대해 월별 데이터이다. 배당금 지급여부는 일일데이터보다 월별데이터로 파악이 더 쉽기 때문이다.)
							  https://finance.yahoo.com/quote/COKE/history/?frequency=1mo&period1=99153000&period2=1733789598

배당금데이터 스크래핑 해와 콘솔에 출력하기


< 야후파이낸셜 코카콜라 배당금 정보 스크래핑 구현하기 >
1. 발생오류 : Caused by: org.jsoup.HttpStatusException: HTTP error fetching URL. Status=503, URL=https://finance.yahoo.com/quote/COKE/history/?frequency=1mo&period1=99153000&period2=1733789598
   원인 : 503 오류는 일반적으로 스크랩하려는 웹사이트가 비인간 사용자가 사이트를 탐색하는 것을 원하지 않기 때문에 차단한다는 것을 의미.
   해결방법 : userAgent 정보 추가해 전달.
   참고페이지 : https://stackoverflow.com/questions/51673995/jsoup-http-error-fetching-url-status-503





=============================== << 서비스 api 설계 >> ===============================
< api 인터페이스 설계 >
요청이 들어오면 서버에서는 그에 대한 응답 내려주거나 응답이 없다면 에러를 내려줄 수 있다.
이 때 어떻게 요청을 보내고 어떻게 응답을 줄 지 결정하는 것이 api 인터페이스 설계이다.
- 요청 : GET /dividend/{company}
- 응답 : {
            companyName: ...,
            dividends: [...]
        }

< 배당금 api 설계 >
1. 특정 회사의 배당금 조회 (핵심, 기본)
   - 방법1. 사용자접근시 요청 : 클라이언트가 해당 데이터에 접근할 때(회사를 클릭할 때) 클라이언트에서 서버로 요청.
   - 방법2. 클라이언트 캐싱 : 캐싱을 통해 회사 데이터를 클라이언트에 저장해두고 필요할 때 꺼내쓸 수 있음.
                          서버까지 요청을 보내고 응답을 받는 과정 생략 -> 속도향상
                          해당 데이터를 저장해야하는 만큼 클라이언트에 추가 저장공간 필요.
                          (속도와 메모리공간은 트레이드오프관계를 지니게됨.)
   요청 : GET /finance/dividend/{companyName}
   응답 : {
           companyName: ..., // 회사명
           dividends: [
                        {
                            date: "2022.3.21"
                            price: "2.00"
                            ...
                        },
                        ...
           ]  // 배당금정보 (다건이므로 리스트형태)
          }

2. 배당금검색 (자동완성기능 추가)
   : 티커(회사코드) 또는 회사명으로 검색가능. (우리는 회사명으로 검색가능)
   : 키워드를 입력할 때마다 자동완성 api 호출.
     (키워드에 적합한 모든 회사정보를 내려줄 필요는 없다.
      적당히 자동검색화면에 출력될 정도만큼의 데이터만 내려주면 된다.
      현재 프로젝트에서는 0 ~ 15개 정도.)
   요청 : GET /company/cutocomplete?keyword=O
   응답 : {
            result: ["O", "OAS", ...]
        }

3. 회사 리스트 조회
   요청 : GET /company
   응답 : {
            result: [
                {
                    companyName: "좋은회사"
                    ticker: "GOOD"
                },
                ...
            ]
        }

4. 배당금 데이터 저장 (관리자기능)
   : 배당금 조회를 하려면 서버의 DB에 해당 데이터가 저장되어 있어야한다.
     따라서 배당금 정보를 조회하는 api가 존재해야 한다.
     이는 고객용이 아닌 서비스 관리자를 위한 기능이다.
     모든 파이낸스 정보를 가져와서 저장하지는 않는다.
     (스크래핑의 시간도 오래 걸리고 데이터의 규모가 너무 커진다.
      따라서 원하는 몇개의 회사의 정보만 스크래핑하고 서비스를 제공하는 컨셉으로 잡는다.)
   요청 : POST /company
         {
            ticker: "GOOD"
         }
   응답 : {
            ticker: "GOOD",
            companyName: "좋은회사",
            ...
        }

4. 배당금 데이터 삭제 (관리자기능)
   요청 : DELETE /company?ticker=GOOD
   응답 : 추가적인 응답값을 내려주지 않고, 응답상태코드로 삭제여부를 전달한다.

5. 회원 API (추후 설계)
   : 인증정보를 어떻게 주고 받는지에 대한 이해가 필수적으로 필요하다.
   - 회원가입
   - 로그인
   - 로그아웃





=============================== << 환경설정 >> ===============================
lombok을 추가해줬는데 실행이 안되면
File > Settings > Build, Exception, Deployment > Compiler > Annotation Processors > Enable annotation processing 이 체크되어있는지 확인





=============================== << 개발관련 조언(?) >> ===============================
- 서버개발 시 중요한 것 중 하나는
  내가 개발하는 서비스가 어떻게 돌아가고 있는지 잘 파악하고 있는 것은 당연하고
  내 서비스가 연동되어 돌아가고 있는 다른 서비스, 서버에 어느정도의 영향력을 갖고 있는지 파악하는 것도 굉장히 중요한 이슈 중 하나이다.
  (내가 개발한 서비스를 어디서 호출하는지, 내가 호출하는 서비스는 어떤 서버인지 가능한 파악하고 있는게 장애가 발생했을 때 당황하지 않고 빠르게 수습 가능하다.)

- 제로베이스 배당금프로젝트 스크래핑 구현01 강의 -> API 보는 법

- 내가 다루는 데이터의 속성을 파악할 수 있어야 한다.
  추후 어떤 도메인의 데이터를 다루게 되더라도 해당 데이터의 특징을 직접 파악하고 그에 맞는 개발을 하고 운영방침을 세워나갈 수 있다.
  내가 담당하는 도메인을 잘 파악하는 것도 중요한 과업이다.

- url 경로는 어떤 데이터를 조회, 조작할 것인지 예측 가능한 정보를 표시하면 좋다.
  무작위로 생성하면 해당 api를 호출하는 사용자가 해당 api가 어떤 기능을 하는지 알 수 없기 떄문이다.

- HTTP 프로토콜 메서드 차이 (꼭 한번 읽어볼 것)
  https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.1
  - GET, POST 차이!!!

- api 구현보다 설계가 중요한 이유.
  구현에 있어서 결과도 중요하지만
  초기에는 왜 이렇게 구현했는지 이해하고 설명할 수 있는 것이 훨씬 중요하다.
  이 과정이 없으면 프로젝트를 찍어내는 것 밖에 안된다.
  면접 과정에서도 왜 이렇게 구현했는지, 왜 해당 라이브러리를 사용했는지, 왜 api는 해당 방식으로 설계했는지 등
  어디까지 고민해보고 코드를 구현했는지가 더 중요하다.

