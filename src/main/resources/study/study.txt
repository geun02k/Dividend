<< 배당금 프로젝트 >>

=============================== << 스크래핑 >> ===============================

< 웹 스크래핑 >
: HTML 문서를 받음 -> 문서를 파싱 -> 필요한 데이터 추출 하는 과정.
  robots.txt에 정의된 규칙을 준수해 스크래핑 해야하고
  요청 서버에 무리가 가지 않는 선에서 요청을 해야한다. (요청의 양, 요청 텀 적절히 설정해야함.)

- 스크래핑을 위해서는 주로 java보다는 파이썬을 사용한다. (파이썬에서 지원되는 라이브러리나 언어 특성상 스크래핑 코드가 간결하기 때문.)

- 데이터를 긁어다 쓰는 게 문제가 되는 이유
  1. 아무 사이트에서나 데이터를 가져오면 안된다.
     데이터는 회사의 중요 자산이기에 소송에 휘말릴 수도 있기 때문이다.

  2. 웹 서버의 부하 측면 문제 발생
     : 스크래핑 서비스는 내 서버(A)에서 다른 서버(B)로 요청을 보내고
	   해당 요청에 대한 응답인 html 문서를 받아오는 것으로 시작한다.
	   이 때 B서버는 서버에서 A요청에 대한 응답을 내려주기 위해 내부적으로 로직을 수행하게 된다.
	   요청 수가 적으면 문제가 되지 않겠지만 요청 수가 몰리게 되면 당연히 요청을 받는 B서버는 부하가 갈 수 밖에 없다.
	   우리가 스크래핑을 위해 요청을 보낸 탓에 B서버에 부하가 가게 되면
	   해당 서비스를 이용중인 다른 사용자에게의 응답이 지연되거나 응답을 받지 못할 수 있다.
	   최악의 경우는 B서버가 트래픽을 견디지 못해 서버가 다운될 수도 있다.
	   (=디도스공격: 서버에 악의적으로 다량의 요청을 보내 트래픽을 폭주시켜 서버가 견디지 못해 다운되도록 하는 공격방식)
	   따라서 B서버 입장에서는 정상적이지 않은 트래픽을 막아야 한다.
	   비정상적인 트래픽이 감지되면 요청을 보낸 IP가 더 이상 요청을 보내지 못하도록 차단해버리기도 한다.
	   따라서 스크래핑을 할 때는 요청을 보내는 B서버에 부하가 가지 않을 정도로만 진행해야 한다.
	   을 DB에 저장한다.

- 긁어와도 되는 데이터인지 확인하는 방법
  : 일반적으로는 robots.txt 문서에 해당 규칙을 정의해 놓는다.
    (웹사이트의 /(root)경로에서 확인가능.
	 ex> google.com/robots.txt)
    트래픽이 제한되어 있거나 다른 서비스에서 데이터를 가져가길 원치 않는 경우에는
	해당 파일에 Disallow 라고 해서 어떤 경로의 접근을 막는지 명시되어 있다.
	- User-agent: * -> 유저 에이젼트에 * 표시가 있는 경우는 해당 규칙이 모든 robots에 대해 적용된다는 의미이다.
	- Disallow : 해당 경로의 접근 비허용.
	- Allow :해당 경로의 접근 허용.
  : robots.txt 문서가 없는 사이트도 있는데 없더라도 스크래핑을 하려면 보수적인 정책을 가지고 진행해야 한다.


스크래핑은 무조건 데이터를 가져오는 게 아니라 꼭 주의사항을 지키면서 가져와야 한다.
우리가 가져오려는 정보는 웹브라우저에서 랜더링된 그래픽 정보가 아닌 실제 웹 사이트가 가진 정보들이다.
이를 내가 사용하려는 형태로 가공해 사용하는 것이 목적이다.

예시)
원하는 회사 목록이 있다.
이 떄 회사들 리스트를 만들어 두고 해당 회사들의 채용공고가 올라오는지를 매번 웹페이지에 들어가 확인하는 것은 번거롭다.
그렇기에 내가 원하는 회사들의 채용공고만 모아서 한번에 다 볼 수 있도록 하고싶다 또는 새로운 채용 공고가 뜨면 그때만 알림을 주면 좋겠다 라는 니즈가 생길 수 있다.
이 때 웹 사이트들의 정보를 긁어 모으는 것을 웹 스크래핑이라 한다.
하지만 이렇게 무조건적으로 웹 사이트들에서 데이터를 긁어오는 것은 문제가 될 수 있다.


< 스크래핑 사용 이유 & 활용분야 >
국내 주식정보 같은 경우는 공공데이터 API를 통해서 데이터가 제공되는 경우가 많다.
해당 API를 활용해서 개발하게 되면 단순 주식정보를 제공하기 위한 서비스 구현은 매우 쉽다.
이는 매우 기본적인 부분이기에 중요하다.

그럼에도 불구하고 스크래핑이라는 번거로운 작업을 하는 이유는 스크래핑을 활용하면
추후 관련 많은 서비스를 만들 수 있기 때문이다.
쇼핑몰의 정보를 스크래핑해서 가격정보를 비교하거나
특정 카테고리에 해당하는 물품을 모아서 보여주거나
여러 회사의 취업공고를 모아서 보여주는 등의 서비스가 가능하게 된다.
또한 업무자동화에서도 유용하게 사용 가능하다.
많은 핀테크 회사들에서도 사용자의 정보 제공 동의를 받아서
사용자 대신 공공기관에 대해 스크래핑 해오고
사용자가 일일이 서류를 발급받아 제공해야하는 번거로움을 줄이는데 활용하기도 한다.


< 스크래핑을 위한 웹사이트 분석 >
1. 야후 파이낸스 웹페이지 접속
   -> 코카콜라 배당금 정보를 가져오기 위해 'coke'검색 > Historical Data 확인
   : https://finance.yahoo.com/quote/COKE/history/

2. robots.txt 문서 확인
   : https://finance.yahoo.com/robots.txt
   -> /quote 경로의 disallow 여부 확인.
      : 해당 경로를 허용하지 않는다는 정보 없음을 확인.

3. 추출할 데이터의 html상 위치 확인.
   - html 확인하기 : 웹페이지 우클릭 > 검사 > Elements 창을 켜고 Dividend 정보에 커서를 가져가 html상에 해당 데이터의 위치 확인.
   - url이 가지는 정보 확인 : 어떤 파라미터가 어떤 정보를 가지는지 확인
							  (아래의 url은 해당정보를 전체 년도에 대해 월별 데이터이다. 배당금 지급여부는 일일데이터보다 월별데이터로 파악이 더 쉽기 때문이다.)
							  https://finance.yahoo.com/quote/COKE/history/?frequency=1mo&period1=99153000&period2=1733789598

배당금데이터 스크래핑 해와 콘솔에 출력하기


< 야후파이낸셜 코카콜라 배당금 정보 스크래핑 구현하기 >
1. 발생오류 : Caused by: org.jsoup.HttpStatusException: HTTP error fetching URL. Status=503, URL=https://finance.yahoo.com/quote/COKE/history/?frequency=1mo&period1=99153000&period2=1733789598
   원인 : 503 오류는 일반적으로 스크랩하려는 웹사이트가 비인간 사용자가 사이트를 탐색하는 것을 원하지 않기 때문에 차단한다는 것을 의미.
   해결방법 : userAgent 정보 추가해 전달.
   참고페이지 : https://stackoverflow.com/questions/51673995/jsoup-http-error-fetching-url-status-503





=============================== << 스크래핑 api 설계 >> ===============================





=============================== << 환경설정 >> ===============================
lombok을 추가해줬는데 실행이 안되면
File > Settings > Build, Exception, Deployment > Compiler > Annotation Processors > Enable annotation processing 이 체크되어있는지 확인





=============================== << 개발관련 조언(?) >> ===============================
- 서버개발 시 중요한 것 중 하나는
  내가 개발하는 서비스가 어떻게 돌아가고 있는지 잘 파악하고 있는 것은 당연하고
  내 서비스가 연동되어 돌아가고 있는 다른 서비스, 서버에 어느정도의 영향력을 갖고 있는지 파악하는 것도 굉장히 중요한 이슈 중 하나이다.
  (내가 개발한 서비스를 어디서 호출하는지, 내가 호출하는 서비스는 어떤 서버인지 가능한 파악하고 있는게 장애가 발생했을 때 당황하지 않고 빠르게 수습 가능하다.)

- 제로베이스 배당금프로젝트 스크래핑 구현01 강의 -> API 보는 법

- 내가 다루는 데이터의 속성을 파악할 수 있어야 한다.
  추후 어떤 도메인의 데이터를 다루게 되더라도 해당 데이터의 특징을 직접 파악하고 그에 맞는 개발을 하고 운영방침을 세워나갈 수 있다.
  내가 담당하는 도메인을 잘 파악하는 것도 중요한 과업이다.




